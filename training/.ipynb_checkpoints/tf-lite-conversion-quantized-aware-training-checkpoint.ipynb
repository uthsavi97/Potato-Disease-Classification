{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6734b23",
   "metadata": {},
   "source": [
    "# Potato Disease Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffee915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "!pip install --upgrade tensorflow-model-optimization\n",
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8facce99",
   "metadata": {},
   "source": [
    "# Set all the Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b05c4bd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlantVillage (copy)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m CHANNELS\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m----> 9\u001b[0m AUTOTUNE \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNEBATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m     10\u001b[0m IMAGE_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[0;32m     11\u001b[0m CHANNELS\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "IMAGE_SIZE = 256\n",
    "default_image_size = tuple((IMAGE_SIZE, IMAGE_SIZE))\n",
    "image_size = 0\n",
    "data_dir = \"PlantVillage (copy)\"\n",
    "CHANNELS=3\n",
    "AUTOTUNE = tf.data.AUTOTUNEBATCH_SIZE = 32\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS=3\n",
    "EPOCHS=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b14613",
   "metadata": {},
   "source": [
    "# Initializing, Exploring & Partioning the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e1d578",
   "metadata": {},
   "source": [
    "# Function to Split Dataset\n",
    "Dataset should be bifurcated into 3 subsets, namely:\n",
    "\n",
    "Training: Dataset to be used while training\n",
    "Validation: Dataset to be tested against while training\n",
    "Test: Dataset to be tested against after we trained a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b91245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    ds_size = ds.cardinality().numpy()\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59dad8c",
   "metadata": {},
   "source": [
    "# Reading and Partitioning the Dataset\n",
    "We create a Tensorflow Dataset Object and directly read it from the directory using image_dataset_from_directory and then split it using the function we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49fcf971",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory ../../PlantVillage (copy)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[0;32m      2\u001b[0m   data_dir,\n\u001b[0;32m      3\u001b[0m   seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m,\n\u001b[0;32m      4\u001b[0m   image_size\u001b[38;5;241m=\u001b[39mdefault_image_size,\n\u001b[0;32m      5\u001b[0m   batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      9\u001b[0m train_ds, val_ds, test_ds \u001b[38;5;241m=\u001b[39m get_dataset_partitions_tf(dataset)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\image_dataset.py:210\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[1;32m--> 210\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m dataset_utils\u001b[38;5;241m.\u001b[39mindex_directory(\n\u001b[0;32m    211\u001b[0m     directory,\n\u001b[0;32m    212\u001b[0m     labels,\n\u001b[0;32m    213\u001b[0m     formats\u001b[38;5;241m=\u001b[39mALLOWLIST_FORMATS,\n\u001b[0;32m    214\u001b[0m     class_names\u001b[38;5;241m=\u001b[39mclass_names,\n\u001b[0;32m    215\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m    216\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m    217\u001b[0m     follow_links\u001b[38;5;241m=\u001b[39mfollow_links,\n\u001b[0;32m    218\u001b[0m )\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\dataset_utils.py:542\u001b[0m, in \u001b[0;36mindex_directory\u001b[1;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    541\u001b[0m     subdirs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mlistdir(directory)):\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    544\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:768\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \n\u001b[0;32m    755\u001b[0m \u001b[38;5;124;03mThe list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;124;03m  errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(path):\n\u001b[1;32m--> 768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[0;32m    769\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    770\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    771\u001b[0m       message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find directory \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[0;32m    773\u001b[0m \u001b[38;5;66;03m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    776\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str_any(filename)\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mGetChildren(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[0;32m    778\u001b[0m ]\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Could not find directory ../../PlantVillage (copy)"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  seed=123,\n",
    "  image_size=default_image_size,\n",
    "  batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f8581",
   "metadata": {},
   "source": [
    "# Checking the Available Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d8725c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m class_names \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mclass_names\n\u001b[0;32m      2\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(class_names)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(n_classes, class_names)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class_names = dataset.class_names\n",
    "n_classes = len(class_names)\n",
    "print(n_classes, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c586c",
   "metadata": {},
   "source": [
    "# Displaying Some Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f194d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cea7e2",
   "metadata": {},
   "source": [
    "# Checking Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85645b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5754a9b5",
   "metadata": {},
   "source": [
    "# Cache, Shuffle, and Prefetch the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b470a096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f5606b",
   "metadata": {},
   "source": [
    "# Building the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71175ae0",
   "metadata": {},
   "source": [
    "# Creating a Layer for Resizing and Normalization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc01f518",
   "metadata": {},
   "source": [
    "Before we feed our images to network, we should be resizing it to the desired size. Moreover, to improve model performance, we should normalize the image pixel value (keeping them in range 0 and 1 by dividing by 256). This should happen while training as well as inference. Hence we can add that as a layer in our Sequential Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8be4fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48664e60",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "Data Augmentation is needed when we have less data, this boosts the accuracy of our model by augmenting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9d96ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237024e7",
   "metadata": {},
   "source": [
    "# Checking what is the expected dimension order for channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "batch_input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "chanDim = -1\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "    input_shape = (CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    batch_input_shape = (BATCH_SIZE, CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    chanDim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798cd78a",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "We use a CNN coupled with a Softmax activation in the output layer. We also add the initial layers for resizing, normalization and Data Augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d92c8bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "n_classes = 3\n",
    "\n",
    "model = models.Sequential([\n",
    "    resize_and_rescale,\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.build(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "556a54a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_3 (Sequential)   (32, 256, 256, 3)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (32, 254, 254, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (32, 127, 127, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (32, 125, 125, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (32, 62, 62, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (32, 60, 60, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (32, 30, 30, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (32, 28, 28, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (32, 14, 14, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (32, 12, 12, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPooli  (32, 6, 6, 64)            0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (32, 4, 4, 64)            36928     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPooli  (32, 2, 2, 64)            0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (32, 256)                 0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (32, 64)                  16448     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (32, 3)                   195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183747 (717.76 KB)\n",
      "Trainable params: 183747 (717.76 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade2ebb",
   "metadata": {},
   "source": [
    "# Compiling the Model\n",
    "We use adam Optimizer, SparseCategoricalCrossentropy for losses, accuracy as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c17e9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44eecbf",
   "metadata": {},
   "source": [
    "# Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0bb229fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "54/54 [==============================] - 112s 2s/step - loss: 0.8877 - accuracy: 0.5498 - val_loss: 0.8314 - val_accuracy: 0.6094\n",
      "Epoch 2/50\n",
      "54/54 [==============================] - 104s 2s/step - loss: 0.6118 - accuracy: 0.7546 - val_loss: 0.3966 - val_accuracy: 0.8698\n",
      "Epoch 3/50\n",
      "54/54 [==============================] - 105s 2s/step - loss: 0.4058 - accuracy: 0.8380 - val_loss: 0.2942 - val_accuracy: 0.8750\n",
      "Epoch 4/50\n",
      "54/54 [==============================] - 107s 2s/step - loss: 0.2215 - accuracy: 0.9242 - val_loss: 0.2522 - val_accuracy: 0.9115\n",
      "Epoch 5/50\n",
      "54/54 [==============================] - 105s 2s/step - loss: 0.1514 - accuracy: 0.9473 - val_loss: 0.1899 - val_accuracy: 0.9271\n",
      "Epoch 6/50\n",
      "54/54 [==============================] - 106s 2s/step - loss: 0.1516 - accuracy: 0.9427 - val_loss: 0.2183 - val_accuracy: 0.9271\n",
      "Epoch 7/50\n",
      "54/54 [==============================] - 108s 2s/step - loss: 0.0963 - accuracy: 0.9647 - val_loss: 0.4601 - val_accuracy: 0.8438\n",
      "Epoch 8/50\n",
      "54/54 [==============================] - 113s 2s/step - loss: 0.0900 - accuracy: 0.9664 - val_loss: 0.1514 - val_accuracy: 0.9583\n",
      "Epoch 9/50\n",
      "54/54 [==============================] - 107s 2s/step - loss: 0.1397 - accuracy: 0.9525 - val_loss: 0.1558 - val_accuracy: 0.9323\n",
      "Epoch 10/50\n",
      "54/54 [==============================] - 105s 2s/step - loss: 0.0800 - accuracy: 0.9716 - val_loss: 0.3433 - val_accuracy: 0.9010\n",
      "Epoch 11/50\n",
      "54/54 [==============================] - 99s 2s/step - loss: 0.0796 - accuracy: 0.9693 - val_loss: 0.1193 - val_accuracy: 0.9531\n",
      "Epoch 12/50\n",
      "54/54 [==============================] - 106s 2s/step - loss: 0.0881 - accuracy: 0.9670 - val_loss: 0.4657 - val_accuracy: 0.8802\n",
      "Epoch 13/50\n",
      "54/54 [==============================] - 104s 2s/step - loss: 0.0869 - accuracy: 0.9711 - val_loss: 0.1488 - val_accuracy: 0.9479\n",
      "Epoch 14/50\n",
      "54/54 [==============================] - 104s 2s/step - loss: 0.0804 - accuracy: 0.9682 - val_loss: 0.2003 - val_accuracy: 0.9323\n",
      "Epoch 15/50\n",
      "54/54 [==============================] - 90s 2s/step - loss: 0.0494 - accuracy: 0.9826 - val_loss: 0.0952 - val_accuracy: 0.9740\n",
      "Epoch 16/50\n",
      "54/54 [==============================] - 78s 1s/step - loss: 0.0581 - accuracy: 0.9792 - val_loss: 0.4349 - val_accuracy: 0.9167\n",
      "Epoch 17/50\n",
      "54/54 [==============================] - 78s 1s/step - loss: 0.0487 - accuracy: 0.9855 - val_loss: 0.2996 - val_accuracy: 0.9323\n",
      "Epoch 18/50\n",
      "54/54 [==============================] - 78s 1s/step - loss: 0.0478 - accuracy: 0.9815 - val_loss: 0.1347 - val_accuracy: 0.9479\n",
      "Epoch 19/50\n",
      "54/54 [==============================] - 78s 1s/step - loss: 0.0469 - accuracy: 0.9786 - val_loss: 0.1509 - val_accuracy: 0.9375\n",
      "Epoch 20/50\n",
      "54/54 [==============================] - 78s 1s/step - loss: 0.0515 - accuracy: 0.9809 - val_loss: 0.5167 - val_accuracy: 0.8646\n",
      "Epoch 21/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.0613 - accuracy: 0.9769 - val_loss: 0.1300 - val_accuracy: 0.9635\n",
      "Epoch 22/50\n",
      "54/54 [==============================] - 81s 1s/step - loss: 0.0262 - accuracy: 0.9925 - val_loss: 0.4708 - val_accuracy: 0.9062\n",
      "Epoch 23/50\n",
      "54/54 [==============================] - 80s 1s/step - loss: 0.0592 - accuracy: 0.9774 - val_loss: 0.1050 - val_accuracy: 0.9635\n",
      "Epoch 24/50\n",
      "54/54 [==============================] - 78s 1s/step - loss: 0.0322 - accuracy: 0.9896 - val_loss: 0.3067 - val_accuracy: 0.9375\n",
      "Epoch 25/50\n",
      "54/54 [==============================] - 78s 1s/step - loss: 0.0392 - accuracy: 0.9873 - val_loss: 0.2934 - val_accuracy: 0.9167\n",
      "Epoch 26/50\n",
      "54/54 [==============================] - 79s 1s/step - loss: 0.0773 - accuracy: 0.9745 - val_loss: 0.3282 - val_accuracy: 0.8854\n",
      "Epoch 27/50\n",
      "54/54 [==============================] - 79s 1s/step - loss: 0.0315 - accuracy: 0.9873 - val_loss: 0.1291 - val_accuracy: 0.9583\n",
      "Epoch 28/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0330 - accuracy: 0.9890 - val_loss: 0.0166 - val_accuracy: 0.9948\n",
      "Epoch 29/50\n",
      "54/54 [==============================] - 78s 1s/step - loss: 0.0216 - accuracy: 0.9936 - val_loss: 0.1182 - val_accuracy: 0.9688\n",
      "Epoch 30/50\n",
      "54/54 [==============================] - 78s 1s/step - loss: 0.0644 - accuracy: 0.9751 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0534 - accuracy: 0.9821 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0235 - accuracy: 0.9907 - val_loss: 0.0896 - val_accuracy: 0.9688\n",
      "Epoch 33/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0292 - accuracy: 0.9925 - val_loss: 0.0328 - val_accuracy: 0.9792\n",
      "Epoch 34/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0214 - accuracy: 0.9948 - val_loss: 0.1904 - val_accuracy: 0.9427\n",
      "Epoch 35/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0470 - accuracy: 0.9850 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0658 - accuracy: 0.9780 - val_loss: 0.5982 - val_accuracy: 0.8438\n",
      "Epoch 37/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0598 - accuracy: 0.9769 - val_loss: 0.0199 - val_accuracy: 0.9844\n",
      "Epoch 38/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0373 - accuracy: 0.9878 - val_loss: 0.1597 - val_accuracy: 0.9531\n",
      "Epoch 39/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0441 - accuracy: 0.9855 - val_loss: 0.0659 - val_accuracy: 0.9688\n",
      "Epoch 40/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0397 - accuracy: 0.9878 - val_loss: 0.3190 - val_accuracy: 0.8750\n",
      "Epoch 41/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0484 - accuracy: 0.9832 - val_loss: 0.1752 - val_accuracy: 0.9427\n",
      "Epoch 42/50\n",
      "54/54 [==============================] - 78s 1s/step - loss: 0.0501 - accuracy: 0.9850 - val_loss: 0.0182 - val_accuracy: 0.9948\n",
      "Epoch 43/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0263 - accuracy: 0.9907 - val_loss: 0.0120 - val_accuracy: 0.9948\n",
      "Epoch 44/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0238 - accuracy: 0.9919 - val_loss: 0.0453 - val_accuracy: 0.9844\n",
      "Epoch 45/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0211 - accuracy: 0.9913 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0382 - accuracy: 0.9873 - val_loss: 0.0369 - val_accuracy: 0.9792\n",
      "Epoch 47/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0143 - accuracy: 0.9931 - val_loss: 0.0364 - val_accuracy: 0.9896\n",
      "Epoch 48/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0161 - accuracy: 0.9942 - val_loss: 0.0871 - val_accuracy: 0.9583\n",
      "Epoch 49/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0249 - accuracy: 0.9902 - val_loss: 0.0466 - val_accuracy: 0.9792\n",
      "Epoch 50/50\n",
      "54/54 [==============================] - 77s 1s/step - loss: 0.0219 - accuracy: 0.9931 - val_loss: 0.0158 - val_accuracy: 0.9896\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1,\n",
    "    epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff3085",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12da8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Calculating model accuracy\")\n",
    "scores = model.evaluate(test_ds)\n",
    "print(f\"Test Accuracy: {round(scores[1],4)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b828809",
   "metadata": {},
   "source": [
    "# Create a Quantization Aware Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d2ae8",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/model_optimization/guide/quantization/training_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14158f",
   "metadata": {},
   "source": [
    "# Quantize only the Dense, MaxPool2D, Conv2D Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f7c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_quantization(layer):\n",
    "    if (\n",
    "        isinstance(layer, layers.Dense)\n",
    "        or isinstance(layer, layers.MaxPool2D)\n",
    "        or isinstance(layer, layers.Conv2D)\n",
    "    ):\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8f3a27",
   "metadata": {},
   "source": [
    "# Clone the Model and Make Quantization Aware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5afab1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m annotated_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mclone_model(\n\u001b[1;32m----> 2\u001b[0m     model,\n\u001b[0;32m      3\u001b[0m     clone_function\u001b[38;5;241m=\u001b[39mapply_quantization,\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      6\u001b[0m quant_aware_model \u001b[38;5;241m=\u001b[39m tfmot\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mquantize_apply(annotated_model)\n\u001b[0;32m      7\u001b[0m quant_aware_model\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "annotated_model = tf.keras.models.clone_model(\n",
    "    model,\n",
    "    clone_function=apply_quantization,\n",
    ")\n",
    "\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e969f",
   "metadata": {},
   "source": [
    "# Compile Quantization Aware Model before Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f65208",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_aware_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd21c71",
   "metadata": {},
   "source": [
    "# Fine Tuning the Quantization Aware Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276dfd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_history = quant_aware_model.fit(train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3e8b14",
   "metadata": {},
   "source": [
    "# Evaluate the Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe0276",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Calculating Quant Aware model accuracy\")\n",
    "scores = quant_aware_model.evaluate(test_ds)\n",
    "print(f\"Test Accuracy: {round(scores[1],4)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bca574",
   "metadata": {},
   "source": [
    "# Convert Quanitzation Aware Model to TF Lite Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d704f0c8",
   "metadata": {},
   "source": [
    "Convert the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2757efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "quantized_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268944f2",
   "metadata": {},
   "source": [
    "# Evaluate the TF Lite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tflite_model(dataset, interpreter):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    prediction_digits = []\n",
    "    test_labels = []\n",
    "    for image, label in dataset.unbatch().take(dataset.unbatch().cardinality()):\n",
    "\n",
    "        test_image = np.expand_dims(image, axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "        interpreter.invoke()\n",
    "        \n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        prediction_digits.append(digit)\n",
    "        test_labels.append(label)\n",
    "\n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    accuracy = (prediction_digits == test_labels).mean()\n",
    "    return accuracy\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_tflite_model(dataset, interpreter)\n",
    "\n",
    "print('Quant TFLite test_accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c334a",
   "metadata": {},
   "source": [
    "# Saving the TF Lite Model\n",
    "We append the model to the list of models as a new version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb320c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_version = max([int(i) for i in (os.listdir(\"../tf-lite-models\")+[0])]) + 1\n",
    "\n",
    "with open(\n",
    "    f\"../tf-lite-models/{model_version}.tflite\",\n",
    "    'wb'\n",
    ") as f:\n",
    "    f.write(quantized_tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaa3c60",
   "metadata": {},
   "source": [
    "# Plotting the Inference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65afa7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for images, labels in test_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))        \n",
    "\n",
    "        actual_class = class_names[labels[i]]\n",
    "\n",
    "        test_image = np.expand_dims(images[i], axis=0).astype(np.float32)\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "\n",
    "        predicted_class = class_names[digit]\n",
    "        confidence = np.max(output()[0])*100\n",
    "\n",
    "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
    "        plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
